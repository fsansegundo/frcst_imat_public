---
title: "2025_09_24 SARIMA, lecture notes"
# output: html_document
format:
  html:
    toc: true
    toc-depth: 4
date: "`r Sys.Date()`"
bibliography: "../forecasting.bib"
link-citations: true 
csl: "../apa-6th-edition.csl"
---


# Preliminaries

```{r message=FALSE}
########################################################################################
##############       Forecasting:      SARIMA models        ############################
########################################################################################


library(MLTools)
library(fpp2)
library(readxl)
library(tidyverse)
library(TSstudio)
library(lmtest)  #contains coeftest function
library(tseries) #contains adf.test function
```

### Set working directory


```{r eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

# Car Registration Example

<!-- ![Fig. 9.12 in FPP3: @hyndman2021fpp3](https://otexts.com/fpp3/figs/arimaflowchart.png) -->

#### Load dataset 

```{r}
fdata <- read_excel("CarRegistrations.xls")
```

#### Visualize the first rows:

```{r}
head(fdata)
```

#### Convert character columns to R date type

```{r}
fdata$Fecha <- as.Date(fdata$Fecha, format = "%d/%m/%Y")
head(fdata)
```

#### Order the table by date (using the pipe operator)

```{r}
fdata <- fdata %>% arrange(Fecha)

```

This is the same as: `fdata <- arrange(fdata, Fecha)`

#### Check for missing dates (time gaps)

```{r}
range(fdata$Fecha)
```

Create a complete sequence of months with the same range and compare it to the dates in our data.

```{r}
date_range <- seq.Date(min(fdata$Fecha), max(fdata$Fecha), by = "months")

date_range[!date_range %in% fdata$Fecha]
```

We also check for duplicates

```{r}
fdata %>% 
  select(Fecha) %>% 
  duplicated() %>% 
  which()
```


#### Check for missing data (NA) in time series value columns

```{r}
sum(is.na(fdata$CarReg))
```

#### Convert it to a time series object   

This is monthly data, so the frequency is 12. The series start in January. If it starts in e.g. April we would use start = c(1960, 4)

```{r}
freq <- 12
start_date  <-  c(1960, 1)
y <- ts(fdata$CarReg, start = start_date, frequency = freq)
```


#### Time plot:  

```{r fig.width=12, fig.height=4}
autoplot(y) +
  ggtitle("Car registrations") +
  xlab("Year") + ylab("Number registered (thousands)")
```



## Train/test split

The train/test split method in forecasting needs to take the temporal structure into account, as illustrated in this picture (from Section 3.4 of  @hyndman2018fpp2)

![](https://otexts.com/fpp2/fpp_files/figure-html/traintest-1.png){width=800px} 



```
n <- length(y)
```

We will leave the last 5 years for validation

```{r fig.width=12, fig.height=4}
n_test <- floor(5 * freq)

y_split <- ts_split(y, sample.out = n_test)
y.TR <- y_split$train
y.TV <- y_split$test
```

Alternatively, with `subset`

```{r fig.width=12, fig.height=4}
# Line 39 of Lab4
y.TR <- subset(y, end = length(y) - 5 * 12) 
y.TV <- subset(y, start = length(y) - 5 * 12 + 1)
```

Or even with `head` and `tail`:

```{r}
y.TR <- head(y, length(y) - 5 * 12) 
y.TV <- tail(y, 5 * 12) 
```

In fact it is usually a good idea to check the split using `head` and `tail` to see the last elements of training and the first ones of `test`:

```{r}
tail(y.TR)
```


```{r}
head(y.TV)
```


And let us also visualize the split.

```{r fig.width=12, fig.height=8}
autoplot(y.TR, color = "orange") + 
  autolayer(y.TV, color = "blue")
```

### What if I don't know the sampling rate of the time series?


Donwload the following data file: [fdata2.dat](https://gist.githubusercontent.com/fsansegundo/61b9cd0e5a7fc8d7c404823bd261b1f6/raw/578dcb42f7c2b34d5467ad6562daf1dc18ad8e82/fdata2.dat)

Let us use plots to explore the series

```{r}
fdata2 <- ts(read.table("fdata2.dat", sep = ""))

ggtsdisplay(fdata2, lag.max = 80)
```

The time plot does not clearly show the frequency (in cases like this you should try *zooming in*, though!). But the ACF is more telling, it shows significant correlations at what seems to be lags = 24, 48, 96, ... (so the sampling rate of this data seems to be ... daily,  weekly, ... ?)


Let us try that and emphasize the lags we suspect to be relevant:

```{r}
freq <- 24
ggAcf(fdata2, lag.max = 80)+
  geom_vline(xintercept = (1:4) * freq, color = "green", alpha = 0.25, size=2)
```

Applying a seasonal difference

```{r}
ggtsdisplay(diff(fdata2, lag = freq), lag.max = 5 * freq)
```



## Identification and fitting process

Let us return to the Car Registration example

### Box-Cox transformation

```{r fig.width=12, fig.height=8}
# Line 48 of Lab4
Lambda <- BoxCox.lambda.plot(y.TR, window.width = 12)
```


```{r}
# Line 51 of Lab4
z <- BoxCox(y.TR, Lambda)
```


```{r fig.width=12, fig.height=8}
# Line 39 of Lab4
p1 <- autoplot(z)
p2 <- autoplot(y)
gridExtra::grid.arrange(p1, p2, nrow = 2 )
```

### Differencing

```{r fig.width=12, fig.height=8}
# Line 58 of Lab4
#recall if the ACF decreases very slowly -> needs differentiation
```


```{r fig.width=12, fig.height=8}
ggtsdisplay(z,lag.max = 100)
```

#### Seasonal Differencing

It is generally better to start with seasonal differencing when a  time series exhibits both seasonal and non-seasonal patterns. 

```{r fig.width=12, fig.height=8}
# Line 62 of Lab4
B12z<- diff(z, lag = freq, differences = 1)
```

We use the ACF and PACF to inspect the result

```{r fig.width=12, fig.height=8}
ggtsdisplay(B12z,lag.max = 4 * freq)
```


#### Regular Differencing (without seasonal)


```{r fig.width=12, fig.height=8}
# Line 66 of Lab4
Bz <- diff(z,differences = 1)
```


```{r fig.width=12, fig.height=8}
ggtsdisplay(Bz, lag.max = 4 * freq)
```


#### Both regular & seasonal Differentiation

```{r fig.width=12, fig.height=8}
# Line 70 of Lab4
B12Bz <- diff(Bz, lag = freq, differences = 1)
```


```{r fig.width=12, fig.height=8}
ggtsdisplay(B12Bz, lag.max = 4 * freq)
```

Remember, when you apply both differences the order does not matter. Here we  check that visually:

```{r fig.width=12, fig.height=8}
B_B12z<- diff(B12z, differences = 1)
```


```{r fig.width=12, fig.height=8}
# Line 75 of Lab4
autoplot(B12Bz, color = "blue", size = 2) + autolayer(B_B12z, color = "orange", size = 0.5)
```

#### Model Order (p, d, q)(P, D, Q)_s selection

Now, using the results above, we select both the regular d and the seasonal D orders of differencing
and the ARMA structure, both regular and seasonal

```{r}
p <- 0
d <- 1
q <- 1

P <- 1
D <- 1
Q <- 1
```

### Fit seasonal model with estimated order

```{r fig.width=12, fig.height=8}
# Line 80 (modified) of Lab4
arima.fit <- Arima(y.TR,
                   order=c(p, d, q),
                   seasonal = list(order=c(P, D, Q), period=freq),
                   lambda = Lambda,
                   include.constant = FALSE)
```


## Model Diagnosis

#### Summary of training errors and estimated coefficients


```{r fig.width=12, fig.height=8}
# Line 86 of Lab4
summary(arima.fit)
```

#### Statistical significance of estimated coefficients

```{r fig.width=12, fig.height=8}
# Line 87 of Lab4
coeftest(arima.fit)
```

#### Root plot

```{r fig.width=12, fig.height=8}
# Line 88 of Lab4
autoplot(arima.fit)
```

#### Check residuals

```{r fig.width=12, fig.height=8}
# Line 91 of Lab4
CheckResiduals.ICAI(arima.fit, bins = 100, lag=100)
```

If residuals are not white noise, change order of ARMA

#### Check the fitted values 

```{r fig.width=12, fig.height=8}
# Line 94 of Lab4
autoplot(y.TR, series = "Real") +
  forecast::autolayer(arima.fit$fitted, series = "Fitted")
```


## Future forecast and validation



```{r fig.width=12, fig.height=8}
# Line 99 of Lab4
y_est <- forecast::forecast(y.TR, model=arima.fit, h=freq)
head(y_est$mean, n = 12)
```


```{r fig.width=12, fig.height=8}
xmin <- 1995
xmax <- 1996
ymin <- 0
ymax <- Inf

autoplot(subset(y, start=length(y.TR) - 30, end=length(y.TR) + freq)) + 
  autolayer(y_est, alpha = 0.5) +
  annotate("rect", xmin=xmin, xmax=xmax, ymin=ymin, ymax= ymax, alpha=0.2, fill="orange")
```

### Validation error for h = 1  

Obtain the forecast in validation for horizon = 1 using the trained parameters of the model. We loop through the validation period, adding one point of the time series each time and forecasting the next value (h = 1).

```{r fig.width=12, fig.height=8}
# Line 104 of Lab4
y.TV.est <- y * NA
```


```{r fig.width=12, fig.height=8}
# Line 106  of Lab4
for (i in seq(length(y.TR) + 1, length(y), 1)){
  y.TV.est[i] <- forecast::forecast(subset(y, end=i-1), 
                          model = arima.fit,       
                          h=1)$mean                
}
```

Next we plot the series and both forecasts. We limit the values depicted in the plot to improve the visualization. As you can see the loop forecasts (with h = 1) appear to be better than the h=12 forecasts in y_est.

```{r fig.width=12, fig.height=8}
# Line 116 of Lab4
autoplot(subset(y, start=length(y.TR) - 24, end = length(y.TR) + 12)) +
  forecast::autolayer(y_est$mean, color="blue") + 
  forecast::autolayer(subset(y.TV.est, start = length(y.TR) + 1, 
                             end = length(y.TR) + 12), color="red")
```

Finally we compute the validation errors

```{r fig.width=12, fig.height=8}
# Line 120 of Lab4
accuracy(subset(y.TV.est, start = length(y.TR) + 1), y)
```

Uncomment the following line to see the direct computation of RSME

```{r fig.width=12, fig.height=8}
sqrt(mean((y.TV.est - y.TV)^2))
```


## Additional Code

We have seen above that the result of using forecast is different from the
result of the validation loop. Both are using the fitted model to forecast,
so what is the difference?

Below we will explicitly use the fitted model coefficients to obtain the
forecasted values, both for the validation loop and for the `forecast`
function. Keep in mind that the Arima function incorporates the BoxCox
transformation. So to make things simpler, we will work directly with
z instead of y.

Recall that the equation of our model is:
$$
(1 - \Phi_1B^{12})(1 - B)(1 - B^{12})z_t =
(1 - \theta_1 B)(1 - \Theta_1 B^{12})\epsilon_t
$$
But we can expand this equation and reorganize it into a forecasting equation:
$$
z_t = z_t = z_{t-1} + z_{t-12} - z_{t-13} +
\Phi_1 (z_{t-12} - z_{t-13} - z_{t-24} + z_{t-25}) + \\
\epsilon_t + \theta_1 \epsilon_{t-1} + \Theta_1 \epsilon_{t-12} +
\theta_1 \Theta_1 \epsilon_{t-13}
$$
Similarly we can do the same with the error term to get
$$
\epsilon_t = z_t - z_{t-1} - z_{t-12} + z_{t-13} -
\phi_1 (z_{t-12} - z_{t-13} - z_{t-24} + z_{t-25}) - \\
\theta_1 \epsilon_{t-1} - \Theta_1 \epsilon_{t-12} - \theta_1 \Theta_1 \epsilon_{t-13}
$$
[Section 9.8 of @hyndman2021fpp3](https://otexts.com/fpp3/arima-forecasting.html#arima-forecasting) explains how to apply this forecasting equations. In summary: 

+ When you start forecasting the first values you replace past values of $z$ with training set values and $\epsilon$ values with the residuals of the fitted model (also corresponding to training). As you move further into the test set, unknown values of $z$ and $\epsilon$ will be needed. 
+ Note that in order to compute $z_t$ using the first equation we need $\epsilon_t$, so we assume that value to be zero. 
+ The main difference between `forecast` and the validation loop is in deciding whether we then use the second equation to update $\epsilon_t$ for $t$ values corresponding to the test set. Let us explore this in the code.

So we begin by splitting z into training and test.

**Technical note:** Lambda was determined using only the training set, and
we apply the same Lambda to the test set to prevent data leakage.

```{r fig.width=12, fig.height=8}
z <- BoxCox(y, Lambda)
z.TR <- subset(z, end = length(y.TR))
z.TV <- subset(z, start = length(y.TR) + 1)
```

We fit a seasonal model to z with the same order we used for y, but setting lambda equal to 1. 

```{r fig.width=12, fig.height=8}
arima.fit <- Arima(z.TR, 
                   order=c(p, d, q),
                   seasonal = list(order=c(P, D, Q), period=12),
                   lambda = 1, 
                   include.constant = FALSE)
```

In the code below you can verify that the model fit is equivalent to what we
obtained above for y.

```{r}
summary(arima.fit)
```


```{r}
coeftest(arima.fit)
```


```{r fig.width=12, fig.height=8}
autoplot(arima.fit)
```

```{r fig.width=12, fig.height=8}
CheckResiduals.ICAI(arima.fit, bins = 100, lag=100)
```

Now let us begin using this model to forecast the test set values.

We begin by using the `forecast` function to forecast **all the values** in the test set. That is, we set the forecasting horizon h equal to the length of the test set,

```{r fig.width=12, fig.height=8}
z_est <- forecast::forecast(arima.fit, h=length(z.TV))
```

Next we will generate three additional versions of the forecasts, using a loop as we did before.

+ The first forecast `z.TV.est` is exactly what we did before, but using z instead of y.
+ The second one `z.TV.est2` will use the two forecasting equations (with the fitted coefficients) and will always use **actual values** of $z_t$ in the test set. The error terms will also be updated with the second forecasting
equation.
+ The third one `z.TV.est3` will also use the forecasting equation, but it will use its own **forecasted values** of $z_t$ in the test set. The error terms for $t$ in the test set will all be set to zero.

First we create empty time series to store the different forecasts.

```{r fig.width=12, fig.height=8}
z.TV.est <- z * NA
z.TV.est2 <- z * NA
z.TV.est3 <- z * NA
```

We make the training values of $z_t$ available to the forecasting equation.

```{r fig.width=12, fig.height=8}
z.TV.est[1:length(z.TR)] <- z.TR
z.TV.est2[1:length(z.TR)] <- z.TR
z.TV.est3[1:length(z.TR)] <- z.TR
```


Similarly, we prepare two versions of $\epsilon_t$ containing the training
residuals, to be used by the second and third procedure. Even though they
look initially the same, these error terms are updated differently: the
second procedure really updates them with a forecast equation, whereas the
third one leaves the test error values as 0.

```{r fig.width=12, fig.height=8}
w2 <- z * 0
w2[1:length(z.TR)] <- residuals(arima.fit)

w3 <- z * 0
w3[1:length(z.TR)] <- residuals(arima.fit)
```

We store the coefficients of the model (`_s` indicates seasonal)

```{r fig.width=12, fig.height=8}
Phi_s <- coefficients(arima.fit)["sar1"]

theta <- coefficients(arima.fit)["ma1"]
Theta_s <- coefficients(arima.fit)["sma1"]
```

And now we get the forecasts in a loop.

```{r fig.width=12, fig.height=8}
for (i in seq(length(z.TR) + 1, length(y), 1)){# loop for validation period
  
  # The first one is simply what we did in the validation loop above
  z.TV.est[i] <- forecast::forecast(subset(z, end=i-1), 
                                    model = arima.fit,  
                                    h=1)$mean           
  # In the second forecast procedure we use the two forecasting equations, with 
  # real values of z and updating the errors with the second equation. 
  z.TV.est2[i] <- z[i - 1] + z[i - 12] - z[i-13] + 
    Phi_s * (z[i-12] - z[i-13] - z[i - 24] + z[i-25]) + 
    w2[i] + theta * w2[i - 1] +  Theta_s * w2[i - 12] + theta * Theta_s * w2[i - 13]
  
  w2[i] = z[i] - z[i-1] - z[i-12] + z[i-13] -
    Phi_s * (z[i-12] - z[i-13] - z[i-24] + z[i-25]) - 
    theta * w2[i-1] - Theta_s * w2[i-12] - theta * Theta_s * w2[i-13]
  
  # And in the third one we update the forecasted values z.TV.est3 using their 
  # previous values and we do not update the error terms (they stay 0)
  z.TV.est3[i] <- z.TV.est3[i - 1] + z.TV.est3[i - 12] - z.TV.est3[i-13] + 
    Phi_s * (z.TV.est3[i-12] - z.TV.est3[i-13] - z.TV.est3[i - 24] + z.TV.est3[i-25]) + 
    w3[i] + theta * w3[i - 1] +  Theta_s * w3[i - 12] + theta * Theta_s * w3[i - 13]

}
```

Let us examine the results, comparing the first values for all the forecast
procedures:

```{r fig.width=12, fig.height=8}
k <- 10
```

Using forecast function directly (we called this s_est above):

```{r fig.width=12, fig.height=8}
head(z_est$mean, k)
```

Using the validation loop as explained in today's session

```{r fig.width=12, fig.height=8}
subset(z.TV.est, start = length(z.TR) + 1, end = length(z.TR) + k)
```

Using the two forecast equations with actual z values and error updates

```{r fig.width=12, fig.height=8}
subset(z.TV.est2, start = length(z.TR) + 1, end = length(z.TR) + k)
```

Using only the forecasting equation for y and no error update

```{r fig.width=12, fig.height=8}
subset(z.TV.est3, start = length(z.TR) + 1, end = length(z.TR) + k)
```


The results indicate that, up to small rounding errors there are only
two different results:

 + the validation loop is doing what we called the second procedure.
 + the `forecast` function is doing what twe called the third procedure.
  
The updated error values and the use of actual values of the time series
explain why the validation loop produces more accurate values. This is
illustrated in the plot below:

```{r fig.width=12, fig.height=8}
autoplot(subset(z, start = length(z.TR) - 30), series="Real") +
  forecast::autolayer(subset(z.TV.est, start = length(z.TR)), series="Forecasting Loop") +
  forecast::autolayer(subset(z.TV.est3, start = length(z.TR)), series="Forecast function")
```

Now we can also compare both approaches through their validation errors. 

```{r fig.width=12, fig.height=8}
accuracy(z_est, z)
accuracy(z.TV.est, z)
```

As expected, direct use of the `forecast` function leads to worse predictive
performance in the test set.


## References


---
nocite: |
  @Krispin2019
---


