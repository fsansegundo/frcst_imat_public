---
title: "2025_09_17 ARMA Processes, lecture notes"
# output: html_document
format:
  html:
    toc: true
    toc-depth: 4
date: "`r Sys.Date()`"
bibliography: "../forecasting.bib"
link-citations: true 
csl: "../apa-6th-edition.csl"
---


```{r message=FALSE}
#####################################################################
##########       Lab Practice 2:      ARMA models         ###########
#####################################################################
```


# Load the required libraries 


```{r message=FALSE}
library(MLTools)
library(fpp2)
library(tidyverse)
library(readxl)
library(lmtest) #contains coeftest function
```

## Set working directory


```{r eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```


# ARMA processes

## Autoregressive (AR) Process Definition in the Non Seasonal (Regular) Case

A stochastic process $(Y_t)$ is called **autoregressive** if it satisfies the equation:

$$y_{t} = \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + \dots + \phi_{p}y_{t-p} + \varepsilon_{t},$$

where:

+ $(\varepsilon_{t})$ is gaussian white noise 
+ $\phi_1, \ldots, \phi_{p}$ are constants, called the **coefficients** of the process. 
+ The number $p$ is the **order** of the process, which is then called an **$AR(p)$ process**.

Any time series which is a realization of an autoregressive process is called an **autoregressive time series**. 

An autoregressive process can be considered as analogous to a multiple linear regression model. But here the prediction of $y_t$ uses recent lags of the time series as input variables. We regress the time series onto its recent past, and that is why we call it **autoregressive**.

#### Backshift Operator and Characteristic Polynomial

Using the **backshift operator** 
$$B(y_t) = y_{t - 1}$$ 

we can write the autoregressive process as:

$$\Phi(B) y_t = (1-\phi_1B - \cdots - \phi_p B^p)y_{t} = \varepsilon_{t}$$

where 

$$\Phi(B) = (1-\phi_1B - \cdots - \phi_p B^p)$$

is the **characteristic polynomial** of the process. 

#### Stationarity in AR Processes

Not all AR processes are stationary: as we have said, a random walk is an example of AR(1) process, but we know it is not stationary. **An autoregressive AR(p) process is stationary if and only if the roots of the characteristic polynomial are outside the unit circle.**



## Generating a pure autoregressive AR(2) time series

**White noise as starting point.** We begin creating a gaussian white noise time series. In order to do that we get a sample of n random values from a standard normal. 

```{r}
n <- 500

set.seed(42)
w <- ts(rnorm(n, mean = 0, sd = 1))
head(w, 25)
```


Now we can use a for loop to generate the autoregressive time series:

```{r}
set.seed(42)
n <- 1000
w <- rnorm(n)

phi <- c(1/3, 1/2)

y <- rep(0, n)

y[1] <- w[1]
y[2] <- -phi[1] * y[1] + w[2]

for (t in 3:n){
  y[t] <- phi[1] * y[t - 1] + phi[2] * y[t - 2] + w[t]
}

y <- ts(y)

autoplot(y)
ggtsdisplay(y, lag.max = min(n/5, 30))
```


### The ACF and PACF of a pure AR(p) model

Given a time series generated by a pure AR(p) process, trying to identify the value of $p$ in the ACF is not easy. That is the reason for the use of the **PACF. Partial autocorrelation** measures the correlation between lagged values in a time series when we remove the influence of correlated lagged values in between.




This ACF is, as we will see, typical of an stationary AR process. Note that the first ACF values drop to zero in an scillatory pattern but they do not suddenly vanish.In fact, we get **three significantly different from zero coefficients.** This observation will be important below in the context of identification of the right modeling strategy for our data. The pattern of the ACF looks more like a slow fading to zero. Observe also the interesting dynamics in the time plot. Both plots indicate that this is certainly not noise. To visualize more examples of this dynamics let us generate some more **realizations of AR(2) processes with the same characteristic polynomial, where the only change is the noise term**. 


## Fitting an ARIMA model to this time series

Now suppose that you know that the series was generated by an AR(2) process, but you don't know the coefficients $\phi_1, \phi_2$. The `Arima` function in the forecast library fits a model to the series, using the order:

```{r}
# Line 32
# Fit model with estimated order
arima.fit <- Arima(y, order=c(2, 0, 0), include.mean = FALSE)
```

We will discuss below the structure of this function call. The coefficients fitted for this model are `ar1, ar2` in the output of the next code line: 

```{r}
summary(arima.fit) # summary of training errors and estimated coefficients
```
And you can see they are not far from the real values.

### Diagnose of the fit. Coefficients and residuals.

The statistical theory behind the ARIMA model allows us to test if the coefficients of the fitted model are significantly different from zero (the null is *the coefficient is 0*):

```{r}
# Line 37
coeftest(arima.fit) # statistical significance of estimated coefficients
```

The small p-values (in the column `Pr(>|z|)` and indicated by the asterisks) show that this is indeed the case. 

Next we check for stationarity, looking at the **inverse values of the roots** of the characteristic polynomial.

```{r}
# Line 39
autoplot(arima.fit) # root plot
```

The *inverse* values are *inside* the unit circle, so that is ok. 

#### Residual diagnosis

The **residuals** of the model are defined as usual
$$
e_t = \hat y_t - y_t
$$
If the ARIMA model is a good fit for the time series, these residuals should behave like gaussian white noise. We can check that with these diagnosis plots:

```{r}
# Line 41
# Check residuals
CheckResiduals.ICAI(arima.fit, bins = 100, lags = 20)
```
The **Ljung-Box** hypothesis test uses the null hypothesis: *the residuals are white noise*. So if we get a low p-value that is a symptom of an unsatisfactory Arima fit. 

Similar plots are obtained with

```{r}
# Line 44
# If residuals are not white noise, change order of ARMA
ggtsdisplay(residuals(arima.fit), lag.max = 20)
```

# Graphic inspection of the fitted values

We can also examine the fitted vs original time series for a visual check of the model fit:

```{r}
# Line 49
# Check fitted forecast
autoplot(y, series = "Real") +
  forecast::autolayer(arima.fit$fitted, series = "Fitted")
```


<!-- ```{r} -->
<!-- y_lags <- ts.intersect(y, y_1 = stats::lag(y, 1), y_2 = stats::lag(y, 2)) -->
<!-- head(y_lags) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- y_lm <- tslm(y ~ y_1 + y_2 - 1, data = y_lags) -->
<!-- summary(y_lm) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- CheckResiduals.ICAI(residuals(y_lm)) -->
<!-- ``` -->


## Moving Average (MA) Process Definition in the Non Seasonal (Regular) Case


### Moving Average Processes

A stochastic process $(Y_t)$ is called a **moving average** process if it satisfies the equation:

$$y_{t} = \varepsilon_t + \theta_{1}\varepsilon_{t-1} + \theta_{2}\varepsilon_{t-2} + \cdots + \theta_{q}\varepsilon_{t-q},$$

where:

+ $\varepsilon_{t}$ is gaussian white noise 
+ $\theta_1, \ldots, \theta_{q}$ are constants, called the **coefficients** of the process. 
+ The number $q$ is the **order** of the process, which is then called an **MA(q) process**.

Any time series which is a realization of a moving average process is called an **moving average time series**. 

The **characteristic polynomial**  of the MA(q) process is:
$$
\Theta(B) = 1 + \theta_{1}B + \theta_{2}B^2 + \cdots + \theta_{q}B^q
$$
and the MA process can be written as:
$$
y_t = \Theta(B) \varepsilon_t
$$

The document [MA_examples](../2025_09_10_StochasticProcess_ARMA/2025_09_10_StochasticProcess_ARMA_files/MA_examples.html) deals with real world examples of htese kind of processes.

#### Invertibility

An $MA(q)$ process is invertible if it can be expressed as an autoregressive process $\text{AR}(\infty)$:
$$
(1 + \theta_1\, B + \theta_1^2\, B^2 + \theta_1^3\, B^2+\dots)y_t = \varepsilon_t
$$

MA(q) processes are invertible if **the roots of the characteristic polynomial $\Theta(B)$ are all outside the unit circle of the complex plane.** 

See e.g. the discussion and the video in Section 9.4 of [@hyndman2021fpp3](https://otexts.com/fpp3/MA.html)




## Generating a pure moving average MA(2) time series


```{r}
set.seed(42)
n <- 1000
w <- rnorm(n)

theta <- c(0.4, -0.3)
# theta <- c(0.4, 0)

y <- rep(0, n)

y[1] <- w[1]
y[2] <- w[2] + theta[1] * w[1] 

for (t in 3:n){
  y[t] <- w[t] + theta[1] * w[t - 1] + theta[2] * w[t - 2]
}

y <- ts(y)

autoplot(y)
ggtsdisplay(y, lag.max = min(n/5, 30))
```

## Definition of the ARMA(p, q) process

An stochastic process is an **ARMA(p, q) process** if it satisfies the equation
$$
y_{t} = \phi_{1}y_{t-1} + \cdots + \phi_{p}y_{t-p} + \theta_{1}\varepsilon_{t-1} + \cdots + \theta_{q}\varepsilon_{t-q} + \varepsilon_{t}
$$
where, as usual, $\varepsilon_{t}$ is white noise, and $(p, q)$ are jointly called the order of the process.

The expression of the ARMA(p, q) process in terms of characteristic polynomials is:
$$
\Phi(B)\,y_t = \Theta(B)\varepsilon_t
$$
And the process is stationary and invertible  if the roots of both $\Phi(B)$ and $\Theta(B)$ are outside the complex unit circle.

### An example of an ARMA(p, q) time series

```{r}
# set.seed(42)
n <- 1000
w <- rnorm(n)

phi <- c(runif(1, -1, 1), 0)
theta <- c(runif(1, -1, 1), 0)

y <- rep(0, n)

y[1] <- w[1]
y[2] <- phi[1] * y[1] + w[2] + theta[1] * w[1] 

for (t in 3:n){
  y[t] <- phi[1] * y[t - 1] + phi[2] * y[t - 2] + w[t] + theta[1] * w[t - 1] + theta[2] * w[t - 2]
}

y <- ts(y)

autoplot(y)
ggtsdisplay(y, lag.max = min(n/5, 30))
```

```{r}
acf_tbl <- acf(y, plot = FALSE, lag.max = min(n/5, 30))
pacf_tbl <- pacf(y, plot = FALSE, lag.max = min(n/5, 30))
head(acf_tbl)
head(pacf_tbl)
```

# Differencing to remove a trend

```{r}
y <- astsa::gtemp_ocean
autoplot(y)
```

```{r}
ggtsdisplay(y)
```

```{r}
d1y <- diff(y, lag = 1)
d1y
```

```{r}
autoplot(d1y)
```

```{r}
ggtsdisplay(d1y)
```


# Converting dates from *last* to *first* day of the month for EDA

This section solves one pending issue from previous sessions, describing an example of the kind of date/time manipulation that is often required when working with time series data. 


```{r echo=FALSE, results='hide'}
library(lubridate)

# Define the start and end dates
start_date <- ymd("1982-01-01")
end_date   <- ymd("1997-12-01")

# Generate sequence of first day of each month
first_days <- seq(from = start_date, to = end_date, by = "month")

# Convert each to last day of month using lubridate::ceiling_date - days(1)
last_days <- ceiling_date(first_days, "month") - days(1)

ndays <- length(last_days)

data_y <- signif(0.005 * 1:ndays + rnorm(ndays), 4)

# autoplot(ts(data_y))

write_csv(tibble(date = last_days, value = data_y), 
          file = "month_final_day.csv")

```

We load the data in this example, from the file *month_final_day.csv*

```{r}
fdata <- read.table("month_final_day.csv", header = TRUE, sep = ",")
head(fdata)
```
As you can see we have monthly data but the dates correspond to the last day of the month. That can be hard to deal with because of the irregularity in the length of the months. Therefore we convert it to first day of the month below. This conversion is **purely for exploratoru purposes**. *If the precise day of data collection is relevant in any way, you need to be extra cautious when doing this.*

First we need to acknowledge the format in the file, converting the column into a `date` object in R:

```{r}
fdata$date <- as.Date(fdata$date, format = "%Y-%m-%d")
head(fdata)
```

Now we can use the `format` function to convert it back to character strings, but with the format of our choice. In the code below we choose to fix the *day* part of the date as `01`. But you could try things like `format = "%Y %b" (try it!) to get different results.

```{r}
fdata$date <- format(fdata$date, format = "%Y-%m-01")
# fdata$date <- format(fdata$date, format = "%Y %b")
head(fdata)
```


Note however that whatever the format you choose, we have fallen back to strings. So for the rest of the EDA we need to apply `as.Date` a second time:

```{r}
fdata$date <- format(fdata$date, format = "%Y-%m-%d")
head(fdata)
```




## References


---
nocite: |
  @hyndman2021fpp3, @Krispin2019
---


